---
title: "DS 202 - Web Scraping 2"
author: "Xiongtao Dai"
ratio: 16x10
output:
  rmdshower::shower_presentation:
    self_contained: false
    katex: true
    theme: ribbon
---
```{r setup, include=FALSE, message=FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

## Web Scraping with R {.shout}


## Automating the process

- functions in R
- loops in R

## Functions in R

- Have been using functions a lot, now we want to write them ourselves!
- Idea: avoid repetitive coding (errors will creep in)
- Instead: extract common core, wrap it in a function, make it reusable

## Structure of functions

- Function name
- Input arguments
    - names, 
    - default values
- Body
- Output values

## A first function

```{r}
mymean <- function(x) {
	return(sum(x)/length(x))
}
```

```{r}
mymean(1:15)
mymean(c(1:15, NA))
```

## A first function (2)

```{r}
mymean <- function(x, na.rm=FALSE) {
	if (na.rm) {
	  x <- na.omit(x)
	}
	return(sum(x)/length(x))
}

mymean(1:15)
mymean(c(1:15, NA), na.rm=TRUE)
```



## Your Turn {.white}

<img class="cover" src="images/blue.jpeg" alt="" width=2000>

<span style="color:white">Connect to the The-Numbers website for weekly boxoffice gross at https://www.the-numbers.com/box-office-chart/weekend/2020/03/13
</span>

<span style="color:white">
<img src="images/green.png" width=20> Use `rvest` to download the box office gross in that week.
</br>
<img src="images/blue.png" width=20> Write a function that uses the url as input argument, scrapes the data, cleans it up, and returns the cleaned data.
</span>

## Your turn - solution

```{r warning = FALSE, message = FALSE}
library(tidyverse)
library(rvest)

url <- "https://www.the-numbers.com/box-office-chart/weekend/2020/03/13"

boxoffice_scraper <- function(url) {
  html <- read_html(url)
  tables <- html %>% html_table()
  box <- tables[[2]]
  names(box)[1:2] <- c("Rank", "Rank.Last.Week")
  box <- box %>% mutate(
    Gross = parse_number(Gross),
    Thr = parse_number(Thr)
  )
  box  
}
```

## Now try it out

```{r, cache =TRUE, message=FALSE}
box <- boxoffice_scraper("https://www.the-numbers.com/box-office-chart/weekend/2020/03/13")
head(box)
```


## Previous week grosses

```{r, message= FALSE}
url <- "https://www.the-numbers.com/box-office-chart/weekend/2020/03/13"
html <- read_html(url)
html %>% html_nodes(".previous a")
html %>% html_nodes(".previous a") %>% html_attr("href")
newurl <- html %>% html_nodes(".previous a") %>% html_attr("href")

newhtml <- read_html(paste0("https://www.the-numbers.com/", newurl))
```
... could repeatedly use this to scrape all previous week grosses.

## Always scrape responsibly! {.shout}

## Loops in R

For-loop: 

```
for (i in 1:n) {
  # expression to be run for each i
  
}
```
but: for-loops sometimes are not the most convenient to write. May have lower efficiency.

## Another example

Clean up baseball career statistics: http://www.baseball-reference.com/players/a/aardsda01.shtml 
Write a function that returns a dataset:

```{r}
bb_scraper <- function(url) {
  html <- read_html(url)

  names <- html %>% html_nodes("h4") %>% html_text()
  values <- html %>% html_nodes(".stats_pullout p") %>% html_text() 
  player <- html %>% html_nodes("h1") %>% html_text()
  data.frame(player=player, 
             statistics=names,  values=parse_number(values))
}
```
## Now try it out

```{r, warning=FALSE}
bb_scraper("http://www.baseball-reference.com/players/a/aardsda01.shtml")
```

## `purrr` package

Includes `map` function, that allows us to run a function on a subgroup or elements of a vector

```{r, warning=FALSE}
html <- read_html("http://www.baseball-reference.com/players/a/")
links <- html %>% html_nodes("#div_players_ a") %>% html_attr("href")

bb <- data.frame(links = paste0("http://www.baseball-reference.com", links), 
                 stringsAsFactors = FALSE)
# get data for the first few players:
bb <- head(bb) %>% mutate(
  data = purrr::map(links, .f = bb_scraper)
)
```

This makes `data` a variable consisting of data frames

## Unpacking the data

```{r, warning=FALSE}
bb1 <- bb %>% unnest(data)
summary(bb1)
```

## Your Turn (5 minutes) {.white}

<img class="cover" src="images/blue.jpeg" alt="" width=2000>

<span style="color:white">Now get the player names, **position**, and career statistics.
</span>

<span style="color:white">
<img src="images/blue.png" width=20> Write a function that uses the url of a player detail page as input argument, scrapes the data, cleans it up, and returns the cleaned data.
</br>
<img src="images/blue.png" width=20> Find the url of the player detail pages of all players with last name starting with B. The list of players is here: https://www.baseball-reference.com/players/b/
</br>
<img src="images/blue.png" width=20> Apply the function to the first 10 urls obtained using `purrr::map`.
</span>

## Your turn solutions

```{r}
bb_scraper2 <- function(url) {
  html <- read_html(url)

  names <- html %>% html_nodes("h4") %>% html_text()
  values <- html %>% html_nodes(".stats_pullout p") %>% html_text() 
  player <- html %>% html_nodes("h1") %>% html_text()
  position <- html %>% html_nodes("h1+ p") %>% html_text()
  names <- trimws(names)
  player <- trimws(player)
  position <- trimws(position)
  data.frame(player=player, 
             position=position,
             statistics=names,  values=parse_number(values))
}
```

## Your turn solutions cont.
```{r, warning=FALSE}
# Get all urls
html <- read_html("http://www.baseball-reference.com/players/b/")
links <- html %>% html_nodes("#div_players_ a") %>% html_attr("href")

bb2 <- data.frame(links = paste0("http://www.baseball-reference.com", 
                                 links[1:10]), 
                 stringsAsFactors = FALSE)

# get data for the first 10 players:
bb2 <- bb2 %>% mutate(
  data = purrr::map(links, .f = bb_scraper2)
)
bb2 <- bb2 %>% unnest(data)
str(bb2)
```


## Your Turn {.white}

<img class="cover" src="images/blue.jpeg" alt="" width=2000>

<span style="color:white">Now, scrape information of the celebrities who appeared in One World: Together at Home.
</span>

<span style="color:white">
<img src="images/blue.png" width=20> Obtain the wikipedia URLs of the celebrities who appeared. Start from [One World: Together at Home](https://en.wikipedia.org/wiki/Together_at_Home#Appearances).
</br>
<img src="images/blue.png" width=20> For each of the first 10 celebrity, use SelectorGadget to select all the paragraph contents.
</br>
<img src="images/blue.png" width=20> Clean the data by removing dubious paragraphs.
</span>
