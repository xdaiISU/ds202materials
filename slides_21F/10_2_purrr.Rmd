---
title: "DS 202 - Web Scraping 2"
author: "Xiongtao Dai"
ratio: 16x10
output:
  rmdshower::shower_presentation:
    self_contained: false
    katex: true
    theme: ribbon
---
```{r setup, include=FALSE, message=FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)
library(tidyverse)
```

## Web Scraping with R {.shout}


## Outline

- functions in R
- loops in R
- using loops to scrape websites

## Functions in R

- Have been using functions a lot, now we want to write one ourselves!
- Idea: avoid repetitive coding (errors will creep in)
- Instead: extract common core, wrap it in a function, make it reusable

## Structure of functions

- Function name
- Input arguments
    - names, 
    - default values
- Body
- Output values

## A first function

```{r}
mymean <- function(x) {
	return(sum(x)/length(x))
}
```

```{r}
mymean(1:15)
mymean(c(1:15, NA))
```

## A first function (2)

```{r}
mymean <- function(x, na.rm=FALSE) {
	if (na.rm) {
	  x <- na.omit(x)
	}
	return(sum(x)/length(x))
}

mymean(1:15)
mymean(c(1:15, NA), na.rm=TRUE)
```


## R demo...


## Case Study {.white}

<img class="cover" src="images/blue.jpeg" alt="" width=2000>

<span style="color:white">Connect to the The-Numbers website for weekly boxoffice gross at https://www.the-numbers.com/box-office-chart/weekend/2021/10/22
</span>

<span style="color:white">
<img src="images/green.png" width=20> Use `rvest` to download the box office gross in that week.
</br>
<img src="images/blue.png" width=20> Write a function that uses the url as input argument, scrapes the data, cleans it up, and returns the cleaned data.
</span>

## Your turn - solution

```{r warning = FALSE, message = FALSE}
library(tidyverse)
library(rvest)

url <- "https://www.the-numbers.com/box-office-chart/weekend/2021/10/22"

boxoffice_scraper <- function(url) {
  html <- read_html(url)
  tables <- html %>% html_table()
  box <- tables[[2]]
  names(box)[1:2] <- c("Rank", "Rank.Last.Week")
  box <- box %>% mutate(
    Gross = parse_number(Gross),
    Thr = parse_number(Thr)
  )
  box  
}
```

## Now try it out

```{r, cache =TRUE, message=FALSE}
box <- boxoffice_scraper("https://www.the-numbers.com/box-office-chart/weekend/2021/10/22")
head(box)
```


## Previous week grosses

```{r, message= FALSE}
url <- "https://www.the-numbers.com/box-office-chart/weekend/2021/10/22"
html <- read_html(url)
html %>% html_nodes(".previous a")
html %>% html_nodes(".previous a") %>% html_attr("href")
newurl <- html %>% html_nodes(".previous a") %>% html_attr("href")

newhtml <- read_html(paste0("https://www.the-numbers.com/", newurl))
```
... could repeatedly use this to scrape all previous week grosses.

## R demo...

## Always scrape responsibly! {.shout}


## Another example

Clean up baseball career statistics of **all players**, starting with: http://www.baseball-reference.com/players/a/aardsda01.shtml 

Write a function that returns a dataset:

```{r}
bb_scraper <- function(url) {
  html <- read_html(url)

  names <- html %>% html_nodes("#info span strong") %>% html_text()
  values <- html %>% html_nodes(".stats_pullout p") %>% html_text() 
  player <- html %>% html_nodes("h1 span") %>% html_text()
  data.frame(player=player, 
             statistics=names,  values=parse_number(values))
}
```
## Now try it out

```{r, warning=FALSE}
bb_scraper("http://www.baseball-reference.com/players/a/aardsda01.shtml")
```

## Loops in R

For-loop: 

```
for (i in 1:n) {
  # expression to be run for each i
  
}
```
but: for-loops sometimes are not the most convenient to write. It is not so expressive sometimes.

## `purrr` package

Includes `map` function, that allows us to run a function on a subgroup or elements of a vector, returning a list. `dplyr::bind_rows` combine a list of data frames by rows.

```{r, warning=FALSE}
html <- read_html("http://www.baseball-reference.com/players/a/")
links <- html %>% html_nodes("#div_players_ a") %>% html_attr("href")
links <- paste0("http://www.baseball-reference.com", links)

# get data for the first few players:
data <- map(links[1:5], .f=bb_scraper) %>%
  bind_rows()
summary(data)
```


## R demo...


## Your Turn (5 minutes) {.white}

<img class="cover" src="images/blue.jpeg" alt="" width=2000>

<span style="color:white">Now get the player names, **position**, and career statistics.
</span>

<span style="color:white">
<img src="images/blue.png" width=20> Write a function that uses the url of a player detail page as input argument, scrapes the data, cleans it up, and returns the cleaned data.
</br>
<img src="images/blue.png" width=20> Find the url of the player detail pages of all players with last name starting with B. The list of players is here: https://www.baseball-reference.com/players/b/
</br>
<img src="images/blue.png" width=20> Apply the function to the first 10 urls obtained using `purrr::map`.
</span>

## Your turn solutions

```{r}
bb_scraper2 <- function(url) {
  html <- read_html(url)

  names <- html %>% html_nodes("#info span strong") %>% html_text()
  values <- html %>% html_nodes(".stats_pullout p") %>% html_text() 
  player <- html %>% html_nodes("h1 span") %>% html_text()
  names <- trimws(names)
  player <- trimws(player)
  data.frame(player=player, 
             statistics=names,  
             values=parse_number(values))
}
```

## Your turn solutions cont.
```{r, warning=FALSE}
# Get all urls
html <- read_html("http://www.baseball-reference.com/players/b/")
links <- html %>% html_nodes("#div_players_ a") %>% html_attr("href")
links <- paste0("http://www.baseball-reference.com", links) 

# get data for the first 5 players:
bb2 <- map(links[1:5], bb_scraper2) %>%
  bind_rows()
str(bb2)
```


<!-- ## Your Turn {.white} -->

<!-- <img class="cover" src="images/blue.jpeg" alt="" width=2000> -->

<!-- <span style="color:white">Now, scrape information of the celebrities who appeared in One World: Together at Home. -->
<!-- </span> -->

<!-- <span style="color:white"> -->
<!-- <img src="images/blue.png" width=20> Obtain the wikipedia URLs of the celebrities who appeared. Start from [One World: Together at Home](https://en.wikipedia.org/wiki/Together_at_Home#Appearances). -->
<!-- </br> -->
<!-- <img src="images/blue.png" width=20> For each of the first 10 celebrity, use SelectorGadget to select all the paragraph contents. -->
<!-- </br> -->
<!-- <img src="images/blue.png" width=20> Clean the data by removing dubious paragraphs. -->
<!-- </span> -->
